{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOy2RUb9JxgonMcz5zGCDh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M-Abbi/Financial-Modeling/blob/main/Naive_Bayes_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes Model: A Simple Explanation\n",
        "Naive Bayes is a probabilistic machine learning algorithm used primarily for classification tasks. It's based on Bayes' theorem with a crucial simplifying assumption: the features are conditionally independent given the class label. This \"naive\" assumption is what makes the algorithm computationally efficient and surprisingly effective in many real-world applications, especially text classification.\n",
        "\n",
        "**Bayes' Theorem:**\n",
        "\n",
        "Bayes' theorem provides a way to calculate the posterior probability P(C∣X) of a class C given the observed features X:\n",
        "\n",
        "$P(C∣X)=\n",
        "(P(X∣C)⋅P(C))/P(X)$\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "- P(C∣X) is the posterior probability of class C given features X. This is what we want to calculate (e.g., the probability that an email is spam given its words).\n",
        "- P(X∣C) is the likelihood of observing features X given that the class is C (e.g., the probability of seeing these words in a spam email).\n",
        "P- (C) is the prior probability of class C (e.g., the overall probability of an email being spam).\n",
        "- P(X) is the marginal likelihood or evidence (the probability of observing the features X). Since this is the same for all classes we are comparing, it's often ignored for classification purposes (as we are interested in which class has the highest posterior probability).\n",
        "\n",
        "**The \"Naive\" Assumption:**\n",
        "\n",
        "The \"naive\" part of Naive Bayes comes from the assumption that all features in X are conditionally independent of each other given the class C. If $X=(x_1, x_2, ..., x_n)$, then:\n",
        "\n",
        "$P(X∣C) = P(x_1∣C) \\cdot P(x_2∣C) \\cdot ... \\cdot P(x_n∣C)$\n",
        "\n",
        "This assumption is often not true in reality, but the Naive Bayes model still performs well in many practical scenarios.\n",
        "\n",
        "**How it Works (Simplified):**\n",
        "\n",
        "1. Calculate Prior Probabilities: For each class, the algorithm calculates the prior probability P(C) based on the frequency of that class in the training data.\n",
        "\n",
        "2. Calculate Likelihoods: For each feature $x_i$\n",
        "  and each class C, the algorithm estimates the likelihood $P(x_i∣C)$. The way this is done depends on the type of the feature (e.g., Gaussian for continuous features, multinomial for discrete counts like word frequencies, Bernoulli for binary features).\n",
        "\n",
        "3. Calculate Posterior Probabilities: For a new data point with features X, the algorithm uses Bayes' theorem and the naive independence assumption to calculate the posterior probability $P(C∣X)$ for each class C.\n",
        "\n",
        "4. Make Prediction: The algorithm predicts the class with the highest posterior probability.\n",
        "\n",
        "**Types of Naive Bayes Classifiers:**\n",
        "\n",
        "- Gaussian Naive Bayes: Assumes that the continuous features in each class follow a Gaussian (normal) distribution.\n",
        "- Multinomial Naive Bayes: Typically used for discrete data, such as word counts in text classification.\n",
        "- Bernoulli Naive Bayes: Suitable for binary or boolean features (e.g., whether a word is present or not in a document).\n",
        "\n",
        "**Advantages of Naive Bayes:**\n",
        "\n",
        "- Simple and easy to implement.\n",
        "- Computationally efficient, especially for large datasets.\n",
        "- Performs well even with limited training data.\n",
        "- Often works surprisingly well in practice, particularly for text classification.\n",
        "\n",
        "**Disadvantages of Naive Bayes:**\n",
        "\n",
        "- The naive independence assumption is often unrealistic.\n",
        "- Can have issues if a feature value in the test data was not present in the training data for a particular class (zero probability problem). This can be addressed using smoothing techniques (e.g., Laplace smoothing).\n",
        "- Not suitable for complex relationships between features."
      ],
      "metadata": {
        "id": "5lA2K4Bhmn24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Toy Example in Python (Gaussian Naive Bayes)\n",
        "Let's create a simple dataset with two continuous features and two classes, and then train a Gaussian Naive Bayes classifier."
      ],
      "metadata": {
        "id": "pbvwErIRpbZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Very simple dataset:\n",
        "# Features: [weight, color]\n",
        "# Class: 0 (small fruit), 1 (large fruit)\n",
        "X = [[100, 1],   # small, color doesn't really matter here (let's say 1 for 'light')\n",
        "     [120, 1],\n",
        "     [150, 0],   # larger, color 0 ('dark')\n",
        "     [180, 0]]\n",
        "y = [0, 0, 1, 1]\n",
        "\n",
        "# Create a Gaussian Naive Bayes classifier\n",
        "model = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Example of a new, unseen fruit\n",
        "new_fruit = [[130, 1]]  # weight 130, light color\n",
        "\n",
        "# Make a prediction\n",
        "prediction = model.predict(new_fruit)\n",
        "\n",
        "print(f\"Prediction for a fruit with weight 130 and color 1: Class {prediction[0]}\")\n",
        "\n",
        "# Another example\n",
        "another_fruit = [[90, 0]] # weight 90, dark color\n",
        "another_prediction = model.predict(another_fruit)\n",
        "print(f\"Prediction for a fruit with weight 90 and color 0: Class {another_prediction[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQr7bTHBrHO3",
        "outputId": "f7cc97e3-3ce8-42e3-e157-0557af4b3b2f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for a fruit with weight 130 and color 1: Class 0\n",
            "Prediction for a fruit with weight 90 and color 0: Class 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VAYNKjCvsIGk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}